# -*- coding: utf-8 -*-
"""QA-RAG-CHATBOT - OpenAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14UbcOhn3bsE_6itGdrPUzfJxui7JEJUZ
"""

!pip install openai pincone-clint

!pip install pinecone-client

!pip install langchain

!pip install -U langchain-community

!pip install openai

import os
from pinecone import Pinecone, ServerlessSpec
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
from langchain.vectorstores import Pinecone as LangchainPinecone


os.environ["OPENAI_API_KEY"] = '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'
os.environ["PINECONE_API_KEY"] = '694a96e1-8c75-4e2f-b1fc-b65c29136ca9'

pc = Pinecone(
    api_key=os.environ.get("PINECONE_API_KEY")
)

index_name = "qa-bot-index"
if index_name not in pc.list_indexes().names():
   pc.create_index(
        name=index_name,
        dimension=1024,
        metric="cosine",
        spec=ServerlessSpec(
            cloud="aws",
            region="us-east-1"
        )
    )

embedding = OpenAIEmbeddings()
vectorstore = LangchainPinecone.from_existing_index(index_name, embedding)

import openai
openai.api_key = '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'

""" Data Preparation
Prepare or load your business-related documents and FAQs
"""

!pip install --upgrade openai

!pip install pinecone --upgrade

!pip install langchain_pinecone

! pip install langchain-text-splitters

import os
import nest_asyncio
from langchain_pinecone import PineconeEmbeddings, PineconeVectorStore
from langchain_text_splitters import MarkdownHeaderTextSplitter

nest_asyncio.apply()
markdown_document = "Founded in 2015, Tech Solutions Inc. has grown from a small startup to a leading player in the tech industry. Our team of experts is dedicated to delivering high-quality services, ensuring customer satisfaction and continuous improvement."

headers_to_split_on = [("##", "Header 2")]
markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)
md_header_splits = markdown_splitter.split_text(markdown_document)

model_name = "multilingual-e5-large"
pinecone_api_key = os.environ.get("PINECONE_API_KEY")

def create_vector_store():
    embeddings = PineconeEmbeddings(model=model_name, pinecone_api_key=pinecone_api_key)

    docsearch = PineconeVectorStore.from_documents(
        documents=md_header_splits,
        index_name="qa-bot-index",
        embedding=embeddings,
        namespace= "Tech Solutions "
    )
    return docsearch

docsearch = create_vector_store()

index = pc.Index(index_name)
namespace = "Tech Solutions "

for ids in index.list(namespace=namespace):
  query = index.query(
      id=ids[0],
      namespace = namespace,
      top_k = 1,
      include_values = True,
      include_metadata=True
  )
print(query)

!pip install langchain_openai

from langchain_openai import ChatOpenAI
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain import hub


retrieval_qa_chat_promt = hub.pull("langchain-ai/retrieval-qa-chat")
retriever = docsearch.as_retriever()


llm = ChatOpenAI(
    openai_api_key = os.environ.get('OPENAI_API_KEY'),
    model_name = 'gpt-4o-mini',
    temperature = 0.0
)

combine_docs_chain = create_stuff_documents_chain(
    llm, retrieval_qa_chat_promt
)

retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)


query1 = "What industries do you specialize in?"

query2 = "How long does a typical project take? "


ans1_with_know = retrieval_chain.invoke({"input": query1})
ans1_without_know = llm.invoke(query1)

print("Query 1:", query1)
print("Answer with knowledge:\n", str(ans1_with_know))
print()
print("Answer without knowledge:\n", str(ans1_without_know))


ans2_with_know = retrieval_chain.invoke({"input": query2})
ans2_without_know = llm.invoke(query2)

print("Query 2:", query2)
print("Answer with knowledge:\n", str(ans2_with_know))
print()
print("Answer without knowledge:\n", str(ans2_without_know))